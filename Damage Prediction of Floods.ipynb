{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Of Damages Caused By Floods & Earthqaukes Using ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "Natural disasters are an increasing phenomenon clearly perceived and known to have a direct, life-altering impact on the welfare of the region it hits and its residents. Depending on where we live, hurricanes, earthquakes, floods, droughts, etc are a threat to lives, properties, productive assets & financial resources. The growing incidence of natural disasters is directly proportional to the increasing vulnerability of households and communities in affected regions. In this work, an artificial neural network has been used to predict the damages caused by natural disasters that can be felt at the community, city and state level as well as on an entire country. Artificial neural networks are mathematical models, inspired by a biological neural network process â€“ the biological neuron. They are used for the modeling of various complex input and output relationships as well as to find and match patterns of any given data. This report results in the comparison of different machine learning algorithms currently used to increase the accuracy of predictions. Training various neural networks, damages occurred due to floods & earthquakes have been estimated using test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Natural disasters cause massive casualties, damages and leave many injured. Human beings cannot stop them but timely prediction and due safety measures can prevent loss of human lives and many precious objects can be saved. The main focus of this project is on the application of data-driven models in the context of real-time forecasting of the damages.\n",
    "\n",
    "This section follows an implementation plan which includes Data Selection, Data Preprocessing and Visualization, Application of Artificial Neural Networks, and its performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Visualzation Packages\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import squarify\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly \n",
    "plotly.tools.set_credentials_file(username='psn1997', api_key='ffj08tmHIdZR3dUcbBIv')\n",
    "\n",
    "# Encoding Packages\n",
    "import category_encoders as ce  #Category Encoder\n",
    "from sklearn.preprocessing import LabelEncoder  #Label Encoder\n",
    "\n",
    "# Preprocessing Packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn import svm  #SVM Model\n",
    "from sklearn.tree import DecisionTreeClassifier  #Decision Tree Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier  #Random Forest Classifier\n",
    "\n",
    "# Artificial Neural Network Models\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense  #FFNN\n",
    "from keras.layers.recurrent import LSTM  #RNN\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from rbflayer import RBFLayer, InitCentersRandom  #RBFN \n",
    "\n",
    "# Evaluation Packages\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix Function \n",
    "\n",
    "A confusion matrix of size n x n associated with a classifier shows the predicted and actual classification, where n is the number of different classes. The prediction accuracy and classification error can be obtained from this matrix.\n",
    "This function prints and plots the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    \n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Function\n",
    "\n",
    "**Precision:** What proportion of positive identifications was actually correct.<br> \n",
    "**Recall:** What proportion of actual positives was identified correctly.<br>\n",
    "**F1 Score:** is needed when you want to seek a balance between Precision and Recall.<br>\n",
    "**Cohen Kappa Score:** Kappa Score is a metric that compares an Observed Accuracy with an Expected Accuracy.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, pred):\n",
    "    class_names = np.array(['0','1','2','3','4'])\n",
    "    plot_confusion_matrix(y_test, pred,classes= class_names , title='Confusion matrix, without normalization')\n",
    "    plt.show()\n",
    "    print(\"Cohen Kappa Score: \"+ str(cohen_kappa_score(y_test, pred)))\n",
    "    print(\"Classification report \\n\" + str(classification_report(y_test, pred, target_names=class_names)))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data\n",
    "\n",
    "Records of floods are obtained from the Storm Events Database (SED), maintained by National Oceanic and Atmospheric Administration's National Weather Service (NWS).\n",
    "\n",
    "The dataset taken into consideration is from the year 2006 till 2018 and contains 16449 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floods = pd.read_csv(\"Project\\database.csv\", index_col=0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visulization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_floods = floods.copy()\n",
    "freq_floods = viz_floods['State'].value_counts()\n",
    "freq_floods = freq_floods.to_frame().reset_index()\n",
    "freq_floods = freq_floods.rename(columns= {\"index\": \"State\", \"State\":\"Frequency\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squarify Plot <br>\n",
    "Flood Prone States of USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = matplotlib.cm.Blues\n",
    "mini=freq_floods['Frequency'][0:10].min()\n",
    "maxi=freq_floods['Frequency'][0:10].max()\n",
    "norm = matplotlib.colors.Normalize(vmin=mini, vmax=maxi)\n",
    "colors = [cmap(norm(value)) for value in freq_floods['Frequency'][0:10]]\n",
    "\n",
    "squarify.plot(sizes=freq_floods['Frequency'][0:10], label=freq_floods['State'][0:10], alpha=.6, color=colors)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotly <br>\n",
    "USA Map for Flood Prone States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "floods_gb=floods.groupby(['State', 'Code']).size()\n",
    "floods_gb = floods_gb.to_frame().reset_index()\n",
    "floods_gb = floods_gb.rename(columns= {0: \"Frequency\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in floods_gb.columns:\n",
    "    floods_gb[col] = floods_gb[col].astype(str)\n",
    "\n",
    "scl = [\n",
    "    [0.0, '#E8EAF6'],\n",
    "    [0.1, '#C5CAE9'],\n",
    "    [0.2, '#9FA8DA'],\n",
    "    [0.3, '#7986CB'],\n",
    "    [0.4, '#5C6BC0'],\n",
    "    [0.5, '#3F51B5'],\n",
    "    [0.6, '#3949AB'],\n",
    "    [0.7, '#303F9F'],\n",
    "    [0.8, '#283593'],\n",
    "    [0.9, '#1A237E'],\n",
    "    [1.0, '#0c1359']\n",
    "]\n",
    "\n",
    "\n",
    "floods_gb['text'] = floods_gb['State']\n",
    "\n",
    "data = [go.Choropleth(\n",
    "    colorscale = scl,\n",
    "    autocolorscale = False,\n",
    "    locations = floods_gb['Code'],\n",
    "    z = floods_gb['Frequency'].astype(float),\n",
    "    locationmode = 'USA-states',\n",
    "    text = floods_gb['text'],\n",
    "    marker = go.choropleth.Marker(\n",
    "        line = go.choropleth.marker.Line(\n",
    "            color = 'rgb(255,255,255)',\n",
    "            width = 2\n",
    "        )),\n",
    "    colorbar = go.choropleth.ColorBar(\n",
    "        title = \"Frequency\")\n",
    ")]\n",
    "\n",
    "layout = go.Layout(\n",
    "    title = go.layout.Title(\n",
    "        text = ''\n",
    "    ),\n",
    "    geo = go.layout.Geo(\n",
    "        scope = 'usa',\n",
    "        projection = go.layout.geo.Projection(type = 'albers usa'),\n",
    "        showlakes = True,\n",
    "        lakecolor = 'rgb(255, 255, 255)'),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "py.iplot(fig, filename = 'd3-cloropleth-map')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing of Data\n",
    "**Discarding Null Values:** Preprocessing on floods dataset involved discarding rows having null values, as these null values are either not supported by many machine learning models or these values caused the output to give a skewed accuracy. <br>\n",
    "**Binary Encoding:** This technique is not as intuitive as the one-hot encoder. In this technique, first the categories are encoded as ordinal, then those integers are converted into binary code, then the digits from that binary string are split into separate columns. This encodes the data in fewer dimensions than one-hot. Category encoders are used to invoke binary encoding functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_floods_drop = floods.copy()\n",
    "final_floods_drop = final_floods_drop.dropna(subset=[\"Damage_Property\"])\n",
    "final_floods_drop = final_floods_drop.dropna(subset=[\"Range_Damage_Property\"])\n",
    "final_floods_drop = final_floods_drop.dropna(subset=[\"Flood_Cause\"])\n",
    "final_floods_drop = final_floods_drop.dropna(subset=[\"Begin_Lat\"])\n",
    "final_floods_drop = final_floods_drop.dropna(subset=[\"Begin_Lon\"])\n",
    "final_floods_drop = final_floods_drop.dropna(subset=[\"End_Lat\"])\n",
    "final_floods_drop = final_floods_drop.dropna(subset=[\"End_Lon\"])\n",
    "print(final_floods_drop.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_floods_ce = final_floods_drop.copy()\n",
    "encoder = ce.BinaryEncoder(cols=['Flood_Cause'])\n",
    "final_floods= encoder.fit_transform(final_floods_ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application of Models\n",
    "\n",
    "This project elaborates six models, which forms the core of our comprehensive comparative study to predict the possible damages caused due to natural disasters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_floods[['Flood_Cause_0', 'Flood_Cause_1', 'Flood_Cause_2', 'Flood_Cause_3','Begin_Lat', 'Begin_Lon', 'End_Lat', 'End_Lon']].copy()\n",
    "y = final_floods[['Range_Damage_Property']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into Train & Test Data  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(y_train['Range_Damage_Property'].value_counts())\n",
    "print(y_test['Range_Damage_Property'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine (SVM) Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVM Model\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print Model Accuracy\n",
    "print(clf.predict(X_test))\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\",max_depth=11)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print Model Accuracy\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest classifer\n",
    "clf=RandomForestClassifier(n_estimators=100,criterion='entropy',max_depth=10,min_samples_split=5,verbose=1)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "# Print Model Accuracy\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Artificial Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covert classes to categorical type\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "\n",
    "encoded_y_train = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_test)\n",
    "\n",
    "encoded_y_test = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feed Forward Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feed Forward Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_dim=8))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the Network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(X_train, encoded_y_train, batch_size=10, epochs=20, verbose=1, validation_data=(X_test, encoded_y_test))\n",
    "\n",
    "# Print Model Accuracy\n",
    "[test_loss, test_acc] = model.evaluate(X_test, encoded_y_test)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))\n",
    "pred = model.predict_classes(X_test)\n",
    "print(pred)\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(y_test, pred)\n",
    "\n",
    "# Print Model Summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrent Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Recurrent Neural Network\n",
    "embed_dim = 128\n",
    "lstm_out = 200\n",
    "batch_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(2500, embed_dim,input_length = X.shape[1], dropout = 0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(5,activation='softmax'))\n",
    "\n",
    "# Compile the Network\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='SGD',metrics = ['accuracy'])\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(X_train, encoded_y_train, batch_size=10, epochs=20, verbose=1, validation_data=(X_test, encoded_y_test))\n",
    "\n",
    "# Print Model Accuracy\n",
    "[test_loss, test_acc] = model.evaluate(X_test,encoded_y_test)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))\n",
    "pred = model.predict_classes(X_test)\n",
    "print(pred)\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(y_test, pred)\n",
    "\n",
    "# Print Model Summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Radial Basis Function Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Radial Basis Function Network\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    X=X_train.values\n",
    "    y=encoded_y_train\n",
    "\n",
    "    model = Sequential()\n",
    "    rbflayer = RBFLayer(16,\n",
    "                        initializer=InitCentersRandom(X), \n",
    "                        betas=1.0,\n",
    "                        input_shape=(8,))\n",
    "    model.add(rbflayer)\n",
    "    model.add(Dense(16, activation='relu', input_dim=8))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the Network\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=RMSprop(), metrics=['accuracy'])\n",
    "    \n",
    "# Train the model using the training sets\n",
    "    model.fit(X, y,\n",
    "              batch_size=10,\n",
    "              epochs=20,\n",
    "              verbose=1)\n",
    "\n",
    "    y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Model Accuracy\n",
    "test_X=X_test.values\n",
    "test_Y=encoded_y_test\n",
    "[test_loss, test_acc] = model.evaluate(test_X,test_Y)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))\n",
    "\n",
    "pred = model.predict_classes(X_test)\n",
    "print(pred)\n",
    "print(\"0.0: \" + str(np.count_nonzero(pred == 0)))\n",
    "print(\"1.0: \" + str(np.count_nonzero(pred == 1)))\n",
    "print(\"2.0: \" + str(np.count_nonzero(pred == 2)))\n",
    "print(\"3.0: \" + str(np.count_nonzero(pred == 3)))\n",
    "print(\"4.0: \" + str(np.count_nonzero(pred == 4)))\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(y_test, pred)\n",
    "\n",
    "# Print Model Summary\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
